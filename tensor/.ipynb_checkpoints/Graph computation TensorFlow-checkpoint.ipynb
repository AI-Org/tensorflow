{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "An introductory notebook to TensorFlow will give an overview of some of the basic concepts of TensorFlow in Python.\n",
    "\n",
    "These will be a good stepping stone to building more complex deep learning networks, such as Convolution Neural Networks, natural language models and Recurrent Neural Networks in the package.  \n",
    "\n",
    "We’ll be creating a simple three-layer neural network to classify the MNIST dataset.  This tutorial assumes that you are familiar with the basics of neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TensorFlow graphs\n",
    "\n",
    "TensorFlow is based on graph based computation – “what on earth is that?”, you might say.  It’s an alternative way of conceptualising mathematical calculations.  Consider the following expression a=(b+c)∗(c+2)a=(b+c)∗(c+2).  We can break this function down into the following components:\n",
    "\n",
    "\\begin{align} \n",
    "d &= b + c \\\\ \n",
    "e &= c + 2 \\\\ \n",
    "a &= d * e \n",
    "\\end{align}\n",
    "Now we can represent these operations graphically as:\n",
    "\n",
    "![simple-graph-computations](images/Simple-graph-computations.png)\n",
    "\n",
    "                    Computational graph\n",
    "\n",
    "This may seem like a silly example – but notice a powerful idea in expressing the equation this way: two of the computations (d=b+cd=b+c and e=c+2e=c+2) can be performed in parallel.  By splitting up these calculations across CPUs or GPUs, this can give us significant gains in computational times.  These gains are a must for big data applications and deep learning – especially for complicated neural network architectures such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs).  The idea behind TensorFlow is to ability to create these computational graphs in code and allow significant performance improvements via parallel operations and other efficiency gains.\n",
    "\n",
    "We can look at a similar graph in TensorFlow below, which shows the computational graph of a three-layer neural network.\n",
    "\n",
    "![TensorFlow-data-flow-graph](images/TensorFlow-data-flow-graph.gif)\n",
    "\n",
    "Here we can see how computational graphs can be used to represent the calculations in neural networks, and this, of course, is what TensorFlow excels at.  Let’s see how to perform some basic mathematical operations in TensorFlow to get a feel for how it all works.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 A Simple TensorFlow example **\n",
    "\n",
    "Let’s first make TensorFlow perform our little example calculation above – a=(b+c)∗(c+2)a=(b+c)∗(c+2).  First we need to introduce ourselves to TensorFlow variables and constants.  Let’s declare some then I’ll explain the syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# first, create a TensorFlow constant\n",
    "const = tf.constant(2.0, name=\"const\")\n",
    "    \n",
    "# create TensorFlow variables\n",
    "b = tf.Variable(2.0, name='b')\n",
    "c = tf.Variable(1.0, name='c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed above, TensorFlow constants can be declared using the tf.constant function, and variables with the tf.Variable function.  The first element in both is the value to be assigned the constant / variable when it is initialised.  The second is an optional name string which can be used to label the constant / variable – this is handy for when you want to do visualisations (as will be discussed briefly later).  TensorFlow will infer the type of the constant / variable from the initialised value, but it can also be set explicitly using the optional dtype argument.  TensorFlow has many of its own types like tf.float32, tf.int32 etc. – see them all [here](https://www.tensorflow.org/api_docs/python/tf/DType)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s important to note that, as the Python code runs through these commands, the variables haven’t actually been declared as they would have been if you just had a standard Python declaration (i.e. b = 2.0).  Instead, all the constants, variables, operations and the computational graph are only created when the initialisation commands are run.\n",
    "\n",
    "Next, we create the TensorFlow operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create some operations\n",
    "d = tf.add(b, c, name='d')\n",
    "e = tf.add(c, const, name='e')\n",
    "a = tf.multiply(d, e, name='a')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow has a wealth of operations available to perform all sorts of interactions between variables, some of which we’ll get to later in the tutorial.  The operations above are pretty obvious, and they instantiate the operations b+cb+c, c+2.0c+2.0 and d∗ed∗e.\n",
    "\n",
    "The next step is to setup an object to initialise the variables and the graph structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the variable initialisation\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we are all set to go.  To run the operations between the variables, we need to start a TensorFlow session – tf.Session.  The TensorFlow session is an object where all operations are run.  Using the with Python syntax, we can run the graph with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable a is 9.0\n"
     ]
    }
   ],
   "source": [
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "    # initialise the variables\n",
    "    sess.run(init_op)\n",
    "    # compute the output of the graph\n",
    "    a_out = sess.run(a)\n",
    "    print(\"Variable a is {}\".format(a_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first command within the with block is the initialisation, which is run with the, well, run command.  Next we want to figure out what the variable a should be.  All we have to do is run the operation which calculates a i.e. a = tf.multiply(d, e, name=’a’).  Note that a is an operation, not a variable and therefore it can be run.  We do just that with the sess.run(a) command and assign the output to a_out, the value of which we then print out.\n",
    "\n",
    "Note something cool – we defined operations d and e which need to be calculated before we can figure out what a is.  However, we don’t have to explicitly run those operations, as TensorFlow knows what other operations and variables the operation a depends on, and therefore runs the necessary operations on its own.  It does this through its data flow graph which shows it all the required dependencies. Using the TensorBoard functionality, we can see the graph that TensorFlow created in this little program:\n",
    "\n",
    "![Simple-TensorFlow-graph](https://github.com/AI-Org/tensorflow/blob/master/images/Simple-TensorFlow-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that’s obviously a trivial example – what if we had an array of b values that we wanted to calculate the value of a over?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Placeholder\n",
    "Let’s also say that we didn’t know what the value of the array b would be during the declaration phase of the TensorFlow problem (i.e. before the with tf.Session() as sess) stage.  In this case, TensorFlow requires us to declare the basic structure of the data by using the tf.placeholder variable declaration.  Let’s use it for b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TensorFlow variables\n",
    "b = tf.placeholder(tf.float32, [None, 1], name='b')\n",
    "x = tf.placeholder(tf.float32, [None, 1], name='x')\n",
    "z = tf.multiply(x, e, name='z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we aren’t providing an initialisation in this declaration, we need to tell TensorFlow what data type each element within the tensor is going to be.  In this case, we want to use tf.float32.  The second argument is the shape of the data that will be “injected” into this variable.  In this case, we want to use a (? x 1) sized array – because we are being cagey about how much data we are supplying to this variable (hence the “?”), the placeholder is willing to accept a None argument in the size declaration.  Now we can inject as much 1-dimensional data that we want into the b variable.\n",
    "\n",
    "The only other change we need to make to our program is in the sess.run(a,…) command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have added the feed_dict argument to the sess.run(a,…) command.  Here we remove the mystery and specify exactly what the variable b is to be – a one-dimensional range from 0 to 10.  As suggested by the argument name, feed_dict, the input to be supplied is a Python dictionary, with each key being the name of the placeholder that we are filling.\n",
    "\n",
    "When we run the program again this time we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable a is 9.0\n",
      "Variable a is [[10.]\n",
      " [11.]\n",
      " [12.]\n",
      " [13.]\n",
      " [14.]\n",
      " [15.]\n",
      " [16.]\n",
      " [17.]\n",
      " [18.]\n",
      " [19.]]\n"
     ]
    }
   ],
   "source": [
    "# setup the variable initialisation\n",
    "#init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    # initialise the variables\n",
    "    sess.run(init_op)\n",
    "    # compute the output of the graph\n",
    "    # we already initialized values of b, the feed_dict \n",
    "    # is not used for computing a\n",
    "    a_out = sess.run(a, feed_dict={b: np.arange(10, 20)[:, np.newaxis]})\n",
    "    # compute the output of the graph for z variable, \n",
    "    print(\"Variable a is {}\".format(a_out))\n",
    "    a_out = sess.run(z, feed_dict={z: np.arange(10, 20)[:, np.newaxis]})\n",
    "    print(\"Variable a is {}\".format(a_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code altogther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable a is [[303.]\n",
      " [306.]\n",
      " [309.]\n",
      " [312.]\n",
      " [315.]\n",
      " [318.]\n",
      " [321.]\n",
      " [324.]\n",
      " [327.]\n",
      " [330.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# reset everything to rerun in jupyter\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# first, create a TensorFlow constant\n",
    "const = tf.constant(2.0, name=\"const\")\n",
    "\n",
    "# create TensorFlow variables\n",
    "b = tf.placeholder(tf.float32, [None, 1], name='b')\n",
    "c = tf.Variable(1.0, name='c')\n",
    "\n",
    "# now create some operations\n",
    "d = tf.add(b, c, name='d')\n",
    "e = tf.add(c, 2, name='e')\n",
    "a = tf.multiply(d, e, name='a')\n",
    "\n",
    "# setup the variable initialisation\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "    # initialise the variables\n",
    "    sess.run(init_op)\n",
    "    # compute the output of the graph\n",
    "    a_out = sess.run(a, feed_dict={b: np.arange(100, 110)[:, np.newaxis]})\n",
    "    print(\"Variable a is {}\".format(a_out))\n",
    "    train_writer = tf.summary.FileWriter('tensorboard_graphs')\n",
    "    train_writer.add_graph(sess.graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![main_graph](https://github.com/AI-Org/tensorflow/blob/master/images/main_graph.png)\n",
    "![auxilliary_nodes](https://github.com/AI-Org/tensorflow/blob/master/images/auxilliary_nodes.png)\n",
    "\n",
    "Notice how TensorFlow adapts naturally from a scalar output (i.e. a singular output when a=9.0) to a tensor (i.e. an array/matrix)?  This is based on its understanding of how the data will flow through the graph.\n",
    "\n",
    "Now we are ready to build a basic MNIST predicting neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move on to the next \"Simple Tensorflow Net with 3 Layers for Recognizing Digits\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_tensoreflow",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
